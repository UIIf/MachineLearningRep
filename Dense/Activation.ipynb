{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import erf\n",
    "\n",
    "\n",
    "class Activation:\n",
    "    \"\"\"Base activation class\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._input = None\n",
    "\n",
    "    @property\n",
    "    def input(self):\n",
    "        \"\"\"Returns the last input received by the activation\"\"\"\n",
    "        return self._input\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Computes activation output\n",
    "\n",
    "        Arguments:\n",
    "            x: Input array of shape (`batch_size`, ...)\n",
    "\n",
    "        Returns:\n",
    "            An array of the same shape as `x`\"\"\"\n",
    "\n",
    "        self._input = x\n",
    "        return x\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Computes loss gradient with respect to the activation input.\n",
    "\n",
    "        Arguments:\n",
    "            gradOutput: Gradient of loss function with recpect to the activation output.\n",
    "                An array of the same shape as the array received in `__call__` method.\n",
    "\n",
    "        Returns:\n",
    "            An array of the same shape as `gradOutput`\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class ReLU(Activation):\n",
    "    \"\"\"Implements ReLU activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        Activation.__call__(self, x)\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        x = self._input\n",
    "        return gradOutput * np.where(x > 0, 1, 0)\n",
    "\n",
    "\n",
    "class LeakyReLU(Activation):\n",
    "    \"\"\"Implements LeakyReLU activation layer\"\"\"\n",
    "\n",
    "    def __init__(self, slope: float = 0.03):\n",
    "        \"\"\"Initializes LeakyReLU layer.\n",
    "\n",
    "        Arguments:\n",
    "            slope: the slope coeffitient of the activation.\"\"\"\n",
    "        super().__init__()\n",
    "        self.slope = slope\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        Activation.__call__(self, x)\n",
    "        return np.maximum(x * self.slope, x)\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        x = self._input\n",
    "        return gradOutput * np.where(x > x * self.slope, 1, self.slope)\n",
    "\n",
    "\n",
    "class GeLU(Activation):\n",
    "    \"\"\"Implements GeLU activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        Activation.__call__(self, x)\n",
    "\n",
    "        return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        x = self._input\n",
    "        derf = 2 / np.sqrt(np.pi) * np.exp(-np.power(x / np.sqrt(2), 2))\n",
    "        dx = 0.5 * (1 + (erf(x / np.sqrt(2)) + (x + derf) / np.sqrt(2)))\n",
    "        return gradOutput * dx\n",
    "\n",
    "\n",
    "class SiLU(Activation):\n",
    "    \"\"\"Implements SiLU (swish) activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        Activation.__call__(self, x)\n",
    "        return x * (1 / (1 + np.exp(-x)))\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        x = self._input\n",
    "\n",
    "        sigm = 1 / (1 + np.exp(-x))\n",
    "        return gradOutput * (sigm + x * sigm * (1 - sigm))\n",
    "\n",
    "\n",
    "class Softplus(Activation):\n",
    "    \"\"\"Implements Softplus (SmoothReLU) activation layer\"\"\"\n",
    "\n",
    "    def __init__(self, beta=1, threshold=20):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        Activation.__call__(self, x)\n",
    "        return np.where(\n",
    "            x * self.beta > self.threshold,\n",
    "            x * self.beta,\n",
    "            1 / self.beta * np.log(1 + np.exp(self.beta * x)),\n",
    "        )\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        x = self._input\n",
    "\n",
    "        return gradOutput * np.where(\n",
    "            x * self.beta > self.threshold,\n",
    "            self.beta,\n",
    "            x * np.exp(self.beta * x) / (1 + np.exp(self.beta * x)),\n",
    "        )\n",
    "\n",
    "\n",
    "class ELU(Activation):\n",
    "    \"\"\"Implements ELU activation layer\"\"\"\n",
    "\n",
    "    def __init__(self, alpha: float = 1):\n",
    "        \"\"\"Initializes ELU layer.\n",
    "\n",
    "        Arguments:\n",
    "            alpha: the alpha coeffitient of the activation.\"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        Activation.__call__(self, x)\n",
    "        return np.where(x > 0, x, self.alpha * (np.exp(x) - 1))\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        x = self._input\n",
    "        return gradOutput * np.where(x > 0, 1, self.alpha * x * np.exp(x))\n",
    "\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    \"\"\"Implements Sigmoid activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        Activation.__call__(self, x)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        x = self._input\n",
    "\n",
    "        sigm = self.__call__(x)\n",
    "        return gradOutput * sigm * (1 - sigm)\n",
    "\n",
    "\n",
    "class Tanh(Activation):\n",
    "    \"\"\"Implements Tanh activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        Activation.__call__(self, x)\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        x = self._input\n",
    "        return gradOutput * (1 - self.__call__(x) ** 2)\n",
    "\n",
    "\n",
    "class Softmax(Activation):\n",
    "    \"\"\"Implements Softmax activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Computes Softmax activation output\n",
    "\n",
    "        Arguments:\n",
    "            x: Input array of shape (`batch_size`, `n_features`)\n",
    "\n",
    "        Returns:\n",
    "            An array of the same shape as `x`\"\"\"\n",
    "        Activation.__call__(self, x)\n",
    "        return x / x.sum(axis=-1)[..., None]\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        x = self._input\n",
    "        return gradOutput * (x.sum() - x) / (np.power(x.sum(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_function(function, shape):\n",
    "    r = function()\n",
    "    print(function.__name__)\n",
    "    print(\"Function\", r(np.random.uniform(size=shape)))\n",
    "    print(\"X\", r.input)\n",
    "    print(\"Grad\", r.grad(np.random.uniform(size=shape)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU\n",
      "Function [0.57367105 0.20495396 0.45844234 0.16983032 0.11385525 0.4874076\n",
      " 0.28419979 0.84406364 0.99939451 0.94121038]\n",
      "X [0.57367105 0.20495396 0.45844234 0.16983032 0.11385525 0.4874076\n",
      " 0.28419979 0.84406364 0.99939451 0.94121038]\n",
      "Grad [0.20966514 0.74256248 0.24493337 0.14985843 0.15503913 0.96747571\n",
      " 0.54672906 0.47384094 0.89505301 0.45983365]\n",
      "\n",
      "LeakyReLU\n",
      "Function [0.13937445 0.68792673 0.83564843 0.10755601 0.94100694 0.68959878\n",
      " 0.74835983 0.38009471 0.64697814 0.54173316]\n",
      "X [0.13937445 0.68792673 0.83564843 0.10755601 0.94100694 0.68959878\n",
      " 0.74835983 0.38009471 0.64697814 0.54173316]\n",
      "Grad [0.23684858 0.36350642 0.78746715 0.67532518 0.58847623 0.21699365\n",
      " 0.14368608 0.61609624 0.03163702 0.67522493]\n",
      "\n",
      "GeLU\n",
      "Function [0.60023574 0.22512139 0.20358088 0.01597122 0.70495202 0.24345675\n",
      " 0.26157972 0.71928578 0.75943592 0.5943225 ]\n",
      "X [0.77020943 0.3529061  0.3245684  0.03116748 0.87213249 0.3764617\n",
      " 0.39927538 0.88580664 0.92380839 0.76433314]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only length-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m check_function(ReLU, (\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m      2\u001b[0m check_function(LeakyReLU, (\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m check_function(GeLU, (\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m      4\u001b[0m check_function(SiLU, (\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m      5\u001b[0m check_function(Softplus, (\u001b[38;5;241m10\u001b[39m))\n",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m, in \u001b[0;36mcheck_function\u001b[0;34m(function, shape)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction\u001b[39m\u001b[38;5;124m\"\u001b[39m, r(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(size\u001b[38;5;241m=\u001b[39mshape)))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, r\u001b[38;5;241m.\u001b[39minput)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrad\u001b[39m\u001b[38;5;124m\"\u001b[39m, r\u001b[38;5;241m.\u001b[39mgrad(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(size\u001b[38;5;241m=\u001b[39mshape)))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "Cell \u001b[0;32mIn[19], line 83\u001b[0m, in \u001b[0;36mGeLU.grad\u001b[0;34m(self, gradOutput)\u001b[0m\n\u001b[1;32m     81\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input\n\u001b[1;32m     82\u001b[0m derf \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mpower(x \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m---> 83\u001b[0m dx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m (erf(x \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m+\u001b[39m (x \u001b[38;5;241m+\u001b[39m derf) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m)))\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gradOutput \u001b[38;5;241m*\u001b[39m dx\n",
      "\u001b[0;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "check_function(ReLU, (10))\n",
    "check_function(LeakyReLU, (10))\n",
    "check_function(GeLU, (10))\n",
    "check_function(SiLU, (10))\n",
    "check_function(Softplus, (10))\n",
    "check_function(ELU, (10))\n",
    "\n",
    "check_function(Sigmoid, (10))\n",
    "check_function(Tanh, (10))\n",
    "check_function(Softmax, (10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\text{GELU}}{\\partial x_i}  =\n",
    "                \\frac{1}{2} + \\frac{1}{2}\\left(\\text{erf}(\\frac{x}{\\sqrt{2}}) +\n",
    "                    \\frac{x + \\text{erf}'(\\frac{x}{\\sqrt{2}})}{\\sqrt{2}}\\right)\n",
    "\n",
    "        where :math:`\\text{erf}'(x) = \\frac{2}{\\sqrt{\\pi}} \\cdot \\exp\\{-x^2\\}`.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function [1.25611526e-02 1.17747648e-02 6.25798377e-05 1.43409824e-02\n",
      " 9.38333792e-03 1.79696478e-03 1.88820898e-02 2.21133190e-02\n",
      " 7.16284269e-03 9.75394268e-03]\n",
      "X [0.41870509 0.39249216 0.00208599 0.47803275 0.31277793 0.05989883\n",
      " 0.62940299 0.73711063 0.23876142 0.32513142]\n",
      "Grad [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
