{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "class Activation:\n",
    "    \"\"\"Base activation class\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._input = None\n",
    "\n",
    "    @property\n",
    "    def input(self):\n",
    "        \"\"\"Returns the last input received by the activation\"\"\"\n",
    "        return self._input\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Computes activation output\n",
    "\n",
    "        Arguments:\n",
    "            x: Input array of shape (`batch_size`, ...)\n",
    "\n",
    "        Returns:\n",
    "            An array of the same shape as `x`\"\"\"\n",
    "\n",
    "        self._input = x\n",
    "        return x\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Computes loss gradient with respect to the activation input.\n",
    "\n",
    "        Arguments:\n",
    "            gradOutput: Gradient of loss function with recpect to the activation output.\n",
    "                An array of the same shape as the array received in `__call__` method.\n",
    "\n",
    "        Returns:\n",
    "            An array of the same shape as `gradOutput`\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class ReLU(Activation):\n",
    "    \"\"\"Implements ReLU activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        Activation.__call__(self, x)\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        x = self._input\n",
    "        return gradOutput * np.where(x > 0, 1, 0)\n",
    "\n",
    "\n",
    "class LeakyReLU(Activation):\n",
    "    \"\"\"Implements LeakyReLU activation layer\"\"\"\n",
    "\n",
    "    def __init__(self, slope: float = 0.03):\n",
    "        \"\"\"Initializes LeakyReLU layer.\n",
    "\n",
    "        Arguments:\n",
    "            slope: the slope coeffitient of the activation.\"\"\"\n",
    "        super().__init__()\n",
    "        self.slope = slope\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        Activation.__call__(self, x)\n",
    "        return np.maximum(x * self.slope, x)\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        x = self._input\n",
    "        return gradOutput * np.where(x > x * self.slope, 1, self.slope)\n",
    "\n",
    "\n",
    "class GeLU(Activation):\n",
    "    \"\"\"Implements GeLU activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        self._input = x\n",
    "        return x * norm.cdf(x)\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        x = self._input\n",
    "        return gradOutput * (norm.cdf(x) + x * norm.pdf(x))\n",
    "\n",
    "\n",
    "class SiLU(Activation):\n",
    "    \"\"\"Implements SiLU (swish) activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        Activation.__call__(self, x)\n",
    "        return x * (1 / (1 + np.exp(-x)))\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        x = self._input\n",
    "\n",
    "        sigm = 1 / (1 + np.exp(-x))\n",
    "        return gradOutput * (sigm + x * sigm * (1 - sigm))\n",
    "\n",
    "\n",
    "class Softplus(Activation):\n",
    "    \"\"\"Implements Softplus (SmoothReLU) activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        Activation.__call__(self, x)\n",
    "        return np.log(1 + np.exp(x))\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        x = self._input\n",
    "\n",
    "        return gradOutput * (1 / (1 + np.exp(-x)))\n",
    "\n",
    "\n",
    "class ELU(Activation):\n",
    "    \"\"\"Implements ELU activation layer\"\"\"\n",
    "\n",
    "    def __init__(self, alpha: float = 1):\n",
    "        \"\"\"Initializes ELU layer.\n",
    "\n",
    "        Arguments:\n",
    "            alpha: the alpha coeffitient of the activation.\"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        Activation.__call__(self, x)\n",
    "        return np.where(x > 0, x, self.alpha * (np.exp(x) - 1))\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        x = self._input\n",
    "        return gradOutput * np.where(x > 0, 1, self.alpha * np.exp(x))\n",
    "\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    \"\"\"Implements Sigmoid activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        Activation.__call__(self, x)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        x = self._input\n",
    "\n",
    "        sigm = self.__call__(x)\n",
    "        return gradOutput * sigm * (1 - sigm)\n",
    "\n",
    "\n",
    "class Tanh(Activation):\n",
    "    \"\"\"Implements Tanh activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        Activation.__call__(self, x)\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        x = self._input\n",
    "        return gradOutput * (1 - self.__call__(x) ** 2)\n",
    "\n",
    "\n",
    "class Softmax(Activation):\n",
    "    \"\"\"Implements Softmax activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Computes Softmax activation output\n",
    "\n",
    "        Arguments:\n",
    "            x: Input array of shape (`batch_size`, `n_features`)\n",
    "\n",
    "        Returns:\n",
    "            An array of the same shape as `x`\"\"\"\n",
    "        Activation.__call__(self, x)\n",
    "        e = np.exp(x)\n",
    "        return e / e.sum(axis=1)[:, None]\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        x = self.__call__(self._input)\n",
    "        eye = np.eye(x.shape[1])\n",
    "        return (gradOutput[:, None] @ ((eye - x[:,None]) * x[..., None])).sum(axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_function(function, shape):\n",
    "    r = function()\n",
    "    print(function.__name__)\n",
    "    print(\"Function\", r(np.random.uniform(size=shape)))\n",
    "    print(\"X\", r.input)\n",
    "    print(\"Grad\", r.grad(np.random.uniform(size=shape)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU\n",
      "Function [0.55184521 0.12710795 0.60209008 0.68153449 0.24096814 0.43219638\n",
      " 0.54662913 0.07445127 0.89695869 0.73381592]\n",
      "X [0.55184521 0.12710795 0.60209008 0.68153449 0.24096814 0.43219638\n",
      " 0.54662913 0.07445127 0.89695869 0.73381592]\n",
      "Grad [0.93388775 0.21841784 0.35668377 0.78763933 0.98600588 0.1297962\n",
      " 0.99164659 0.68629503 0.77298396 0.93357807]\n",
      "\n",
      "LeakyReLU\n",
      "Function [0.57558574 0.61860531 0.94181882 0.36701776 0.01626319 0.34122589\n",
      " 0.20685179 0.04144744 0.39378443 0.78579776]\n",
      "X [0.57558574 0.61860531 0.94181882 0.36701776 0.01626319 0.34122589\n",
      " 0.20685179 0.04144744 0.39378443 0.78579776]\n",
      "Grad [0.45986167 0.59500219 0.65174303 0.92246165 0.29287107 0.61945581\n",
      " 0.14220388 0.06823636 0.3387666  0.70116917]\n",
      "\n",
      "GeLU\n",
      "Function [0.11724375 0.01722571 0.62924423 0.51910951 0.29189549 0.59821699\n",
      " 0.5036627  0.80445175 0.32928862 0.34991082]\n",
      "X [0.20211432 0.03355331 0.79875305 0.68817433 0.43647635 0.76813116\n",
      " 0.67220227 0.96580932 0.48091566 0.5048098 ]\n",
      "Grad [0.63141841 0.3341253  0.70248715 0.78056659 0.60610887 0.05099584\n",
      " 0.58982877 0.30550655 0.7000239  0.71811594]\n",
      "\n",
      "SiLU\n",
      "Function [0.09463887 0.24717437 0.26639473 0.67221966 0.12942557 0.26027982\n",
      " 0.04424104 0.40501754 0.49133736 0.00685197]\n",
      "X [0.17415156 0.41104103 0.43826121 0.93588907 0.23204813 0.42965298\n",
      " 0.08488179 0.62237716 0.72847793 0.0136113 ]\n",
      "Grad [0.5243147  0.1889041  0.38558903 0.82370137 0.22157398 0.26756051\n",
      " 0.29457063 0.37907116 0.33973439 0.07484361]\n",
      "\n",
      "Softplus\n",
      "Function [0.89270393 1.09100975 1.2029395  0.8361826  1.31085855 1.26019104\n",
      " 0.72474322 1.07480364 1.16227906 1.27305551]\n",
      "X [0.36583892 0.68172159 0.84582139 0.26814853 0.99671134 0.92667465\n",
      " 0.06222426 0.6572182  0.78718911 0.94458637]\n",
      "Grad [0.1969047  0.13633822 0.24059337 0.32010127 0.21413599 0.25016754\n",
      " 0.01376047 0.54314242 0.4089752  0.66534411]\n",
      "\n",
      "ELU\n",
      "Function [0.52809814 0.72159052 0.51625221 0.54573851 0.86311303 0.25388652\n",
      " 0.68369332 0.64996587 0.39405974 0.96871516]\n",
      "X [0.52809814 0.72159052 0.51625221 0.54573851 0.86311303 0.25388652\n",
      " 0.68369332 0.64996587 0.39405974 0.96871516]\n",
      "Grad [0.89477638 0.73381736 0.60770924 0.34398981 0.30974203 0.35865544\n",
      " 0.41064348 0.77558227 0.94928443 0.1753252 ]\n",
      "\n",
      "Sigmoid\n",
      "Function [0.69923343 0.71943937 0.68647756 0.5864725  0.55647915 0.65821589\n",
      " 0.6060782  0.58469949 0.60670992 0.71618463]\n",
      "X [0.84365019 0.94168242 0.78370259 0.34940176 0.22688487 0.65535371\n",
      " 0.43085662 0.34209562 0.43350331 0.92561409]\n",
      "Grad [0.07705164 0.0435536  0.01151792 0.06981059 0.20191684 0.16544365\n",
      " 0.17928262 0.22407194 0.20343426 0.06097324]\n",
      "\n",
      "Tanh\n",
      "Function [0.58819223 0.07099939 0.01343435 0.19252794 0.67254457 0.60312844\n",
      " 0.35038512 0.24617302 0.29235524 0.64153587]\n",
      "X [0.67489752 0.07111906 0.01343516 0.1949611  0.81537474 0.69804978\n",
      " 0.3658827  0.25133484 0.30113969 0.7607795 ]\n",
      "Grad [0.06213368 0.27615527 0.35825602 0.24241803 0.01391708 0.08527645\n",
      " 0.19158135 0.63317854 0.16910224 0.04192623]\n",
      "\n",
      "Softmax\n",
      "Function [[0.11078157 0.17988907 0.1936055  0.2641606  0.25156327]\n",
      " [0.18122504 0.19038796 0.21964617 0.25578088 0.15295995]\n",
      " [0.23177328 0.23097186 0.18125496 0.24209458 0.11390532]\n",
      " [0.16265316 0.18224791 0.16561031 0.31494276 0.17454586]\n",
      " [0.16996971 0.16320709 0.15575954 0.25691893 0.25414474]\n",
      " [0.13890022 0.23769506 0.21983237 0.26236978 0.14120257]\n",
      " [0.24640561 0.19943441 0.18688367 0.10620205 0.26107425]\n",
      " [0.15369294 0.12365396 0.27813894 0.19301846 0.2514957 ]\n",
      " [0.12910659 0.20979729 0.18490716 0.22791863 0.24827033]\n",
      " [0.15599108 0.19740581 0.13526534 0.33885909 0.17247867]]\n",
      "X [[0.06746494 0.55224495 0.62572711 0.93646178 0.88759907]\n",
      " [0.2936117  0.34293604 0.48589007 0.63819326 0.12404823]\n",
      " [0.81315691 0.80969318 0.56730194 0.85672575 0.10276486]\n",
      " [0.18785929 0.30160711 0.20587673 0.84863013 0.25842674]\n",
      " [0.42389572 0.38329534 0.3365889  0.83703605 0.82617941]\n",
      " [0.20697241 0.74420518 0.66608188 0.84297148 0.22341213]\n",
      " [0.8541708  0.64267719 0.57767811 0.01253521 0.91199664]\n",
      " [0.36144704 0.14397731 0.95461107 0.58927614 0.85391618]\n",
      " [0.3326522  0.81815566 0.69186773 0.90100258 0.98653208]\n",
      " [0.14969255 0.38515529 0.00713204 0.92547808 0.25016732]]\n",
      "Grad [[ 0.02782138 -0.055429   -0.02370352  0.05905114 -0.00773999]\n",
      " [-0.01172594 -0.0454031   0.00846474  0.08998194 -0.04131763]\n",
      " [-0.03232252  0.01990961 -0.00189193  0.05161127 -0.03730642]\n",
      " [-0.04349339 -0.01240124  0.06185684  0.02061018 -0.0265724 ]\n",
      " [-0.01136171  0.05039268  0.0353161   0.03281722 -0.1071643 ]\n",
      " [ 0.03196586 -0.05053035  0.01899599  0.04300818 -0.04343968]\n",
      " [-0.02849681 -0.0315324   0.03899002 -0.03908578  0.06012497]\n",
      " [-0.04273928  0.01442815 -0.09561252  0.11641741  0.00750623]\n",
      " [ 0.00866975  0.03347489 -0.01944308 -0.00016163 -0.02253992]\n",
      " [-0.07568048  0.02010605 -0.01445685  0.07097011 -0.00093884]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_function(ReLU, (10))\n",
    "check_function(LeakyReLU, (10))\n",
    "check_function(GeLU, (10))\n",
    "check_function(SiLU, (10))\n",
    "check_function(Softplus, (10))\n",
    "check_function(ELU, (10))\n",
    "\n",
    "check_function(Sigmoid, (10))\n",
    "check_function(Tanh, (10))\n",
    "check_function(Softmax, (10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\text{GELU}}{\\partial x_i}  =\n",
    "                \\frac{1}{2} + \\frac{1}{2}\\left(\\text{erf}(\\frac{x}{\\sqrt{2}}) +\n",
    "                    \\frac{x + \\text{erf}'(\\frac{x}{\\sqrt{2}})}{\\sqrt{2}}\\right)\n",
    "\n",
    "        where :math:`\\text{erf}'(x) = \\frac{2}{\\sqrt{\\pi}} \\cdot \\exp\\{-x^2\\}`.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function [1.25611526e-02 1.17747648e-02 6.25798377e-05 1.43409824e-02\n",
      " 9.38333792e-03 1.79696478e-03 1.88820898e-02 2.21133190e-02\n",
      " 7.16284269e-03 9.75394268e-03]\n",
      "X [0.41870509 0.39249216 0.00208599 0.47803275 0.31277793 0.05989883\n",
      " 0.62940299 0.73711063 0.23876142 0.32513142]\n",
      "Grad [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
